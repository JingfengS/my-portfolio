<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <style>
        body {
          background-color: white;
          padding: 100px;
          width: 1000px;
          margin: auto;
          text-align: left;
          font-weight: 300;
          font-family: 'Open Sans', sans-serif;
          color: #121212;
          max-width: 100%;
        }
        strong {
          font-weight: 600;
        }
        h1, h2, h3, h4 {
          font-family: 'Source Sans Pro', sans-serif;
        }
        kbd {
          color: #121212;
        }
        code {
          background-color: #f5f5f5;
          padding: 2px 4px;
          border-radius: 4px;
          color: #c7254e;
          font-family: 'Courier New', Courier, monospace;
        }
        pre {
          background-color: #f5f5f5;
          padding: 15px;
          border-radius: 4px;
          overflow-x: auto;
        }
        .tensor-code {
            font-size: 14px; 
        }
        div.padded {
          padding-top: 30px;
        }
        .align-center {
            text-align: center;
        }
        .image-row {
            display: flex;
            justify-content: center;
            gap: 20px;
        }
        .image-container {
            text-align: center;
        }
        img {
            max-width: 400px;
            border: 1px solid #ddd;
            padding: 5px;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin-top: 20px;
        }
        td {
            padding: 10px;
            text-align: center;
            vertical-align: top;
        }
        figcaption {
            margin-top: 5px;
            font-size: 0.9em;
            color: #555;
        }
    </style>
        <title>CS184 Path Tracer Part 1 Write-Up</title>
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <link
            href="https://fonts.googleapis.com/css?family=Open+Sans@wght@300;600|Source+Sans+Pro"
            rel="stylesheet">

        <script
            src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>

    <body>

        <div class="container">
            <h1 align="middle">CS184/284A Spring 2025 Homework 3 Write-Up</h1>
            <div style="text-align: center;">Name: Sun Jingfeng</div>

            <br>

            Link to webpage:
            <a
                href="https://jingfengs.github.io/cs184-284a-sp25-hw-webpages/hw3/index.html">
                https://jingfengs.github.io/cs184-284a-sp25-hw-webpages/hw3/index.html
            </a>

            <br />

            Link to GitHub repository:
            <a href="https://github.com/JingfengS/cs184-284a-sp25-hw-webpages">
                https://github.com/JingfengS/cs184-284a-sp25-hw-webpages
            </a>

            <figure>
                <img src="cornell.png" alt="Cornell Boxes with Bunnies"
                    style="width:70%" />
                <figcaption>Path Tracer</figcaption>
            </figure>

            <h2>Overview</h2>
            <p>
                In this assignment, I implemented the core foundation of a
                physically-based renderer. This involved two major components:
                generating rays from a virtual camera to sample the scene, and
                calculating the intersections between those rays and scene
                primitives (triangles and spheres).
            </p>
            <p>
                I learned how to bridge the gap between 2D image coordinates and
                3D world space using a camera sensor model. Additionally,
                implementing the Möller-Trumbore algorithm gave me a deeper
                appreciation for the mathematical efficiency required in
                graphics, as we solve for barycentric coordinates and
                intersection time in a single step.
            </p>

            <h2>Part 1: Ray Generation and Scene Intersection</h2>

            <h3>Ray Generation Pipeline</h3>
            <p>
                The ray generation process bridges the 2D image space and the 3D
                world space. The goal is to take a normalized image coordinate
                \((x, y)\)—where \((0,0)\) is the bottom-left and \((1,1)\) is
                the top-right of the image—and transform it into a ray
                originating from the camera and shooting into the scene.
            </p>
            <p>
                My implementation in <code>Camera::generate_ray</code> follows
                these steps:
            </p>
            <ol>
                <li>
                    <strong>Camera Space Transformation:</strong>
                    We define a virtual sensor plane at \(Z = -1\) in camera
                    space. The size of this sensor is determined by the
                    horizontal (\(hFov\)) and vertical (\(vFov\)) fields of
                    view. I calculated the sensor dimensions using the tangent
                    of half the FOV angles.
                </li>
                <li>
                    <strong>Coordinate Mapping:</strong>
                    Since the input coordinates \((x, y)\) are normalized to
                    \([0, 1]\), I mapped them to the sensor's coordinate system.
                    Initially, I encountered an issue where the image appeared
                    upside down due to coordinate conventions. I fixed this by
                    mapping the Y-coordinate such that \(0\) maps to the bottom
                    of the sensor (negative) and \(1\) maps to the top
                    (positive):
                    <pre><code>// Transform normalized coordinates to sensor space
double sensor_x = (2 * x - 1) * tan(radians(hFov) / 2);
double sensor_y = (2 * y - 1) * tan(radians(vFov) / 2);</code></pre>
                    This ensures the center of the image \((0.5, 0.5)\) aligns
                    with the camera's optical axis \((0, 0, -1)\).
                </li>
                <li>
                    <strong>World Space Transformation:</strong>
                    The ray's direction vector in camera space is \((sensor\_x,
                    sensor\_y, -1)\). I then multiplied this vector by the
                    camera-to-world rotation matrix (<code>c2w</code>) to orient
                    the ray correctly in the world.
                </li>
                <li>
                    <strong>Ray Creation:</strong>
                    Finally, the ray is created with its origin at the camera's
                    position (<code>pos</code>) and the calculated normalized
                    direction. I also initialized the ray's <code>min_t</code>
                    and <code>max_t</code> using the near (<code>nClip</code>)
                    and far (<code>fClip</code>) clipping planes.
                </li>
            </ol>
            <p>
                To perform supersampling (antialiasing), the
                <code>raytrace_pixel</code> function calls
                <code>generate_ray</code> multiple times per pixel. It uses a
                <code>gridSampler</code> to generate random offsets \((\Delta x,
                \Delta y)\) within the pixel area, averaging the radiance
                results to produce a smoother image.
            </p>

            <h3>Primitive Intersection</h3>
            <p>
                <strong>Triangle Intersection:</strong>
                For ray-triangle intersection, I implemented the
                <strong>Möller-Trumbore algorithm</strong>. This is a fast,
                efficient method that avoids computing the plane equation of the
                triangle explicitly. Instead, it solves for the barycentric
                coordinates \((1-b_1-b_2, b_1, b_2)\) and the ray parameter
                \(t\) simultaneously using Cramer's rule.
            </p>
            <p>
                An intersection is valid only if:
                <ul>
                    <li>The time \(t\) is within the valid range \([min\_t,
                        max\_t]\).</li>
                    <li>The barycentric coordinates are valid: \(b_1 \ge 0\),
                        \(b_2 \ge 0\), and \((b_1 + b_2) \le 1\). This ensures
                        the point lies inside the triangle.</li>
                </ul>
            </p>

            <p>
                <strong>Sphere Intersection:</strong>
                For spheres, I used the analytic geometric approach. I
                substituted the ray equation \(P(t) = O + tD\) into the sphere
                equation \((P - C)^2 - R^2 = 0\), resulting in a quadratic
                equation \(At^2 + Bt + C = 0\). I solved for \(t\) using the
                quadratic formula. If the discriminant is non-negative, the ray
                intersects the sphere, and I selected the closest valid \(t\)
                within the clipping planes.
            </p>

            <h3>Results: Normal Shading</h3>
            <p>
                Below are images of several simple <code>.dae</code> files
                rendered with normal shading to verify the implementation.
            </p>

            <div
                style="display: flex; flex-direction: column; align-items: center;">
                <table
                    style="width: 100%; text-align: center; border-collapse: collapse;">
                    <tr>
                        <td style="text-align: center;">
                            <img src="assets/part1/CBempty.png" width="400px" />
                            <figcaption>CBempty.dae (Box walls)</figcaption>
                        </td>
                        <td style="text-align: center;">
                            <img src="assets/part1/CBspheres.png"
                                width="400px" />
                            <figcaption>CBspheres.dae (Spheres)</figcaption>
                        </td>
                        <td style="text-align: center;">
                            <img src="assets/part1/banana.png" width="400px" />
                            <figcaption>Banana</figcaption>
                        </td>
                    </tr>
                </table>
            </div>
            <h2>Part 2: Bounding Volume Hierarchy</h2>

            <h3>BVH Construction Algorithm</h3>
            <p>
                My BVH construction algorithm is a recursive process that
                organizes scene primitives into a binary tree structure for
                efficient ray intersection. The core function,
                <code>construct_bvh</code>, takes a list of primitives and
                recursively divides them until a leaf node is formed.
            </p>
            <p>
                The detailed steps are as follows:
            </p>
            <ol>
                <li>
                    <strong>Bounding Box Calculation:</strong>
                    First, I compute the bounding box of the current set of
                    primitives. If the number of primitives is small enough
                    (less than or equal to <code>max_leaf_size</code>), I
                    immediately create a leaf node and return.
                </li>
                <li>
                    <strong>Heuristic Selection (SAH):</strong>
                    If the node is an internal node, I employ the
                    <strong>Surface Area Heuristic (SAH)</strong> to determine
                    the optimal splitting plane. I calculate the bounding box of
                    all primitive centroids and choose the axis with the largest
                    extent as the splitting axis.
                </li>
                <li>
                    <strong>Binning Strategy:</strong>
                    To avoid the expensive \( O(N^2) \) cost of testing every
                    possible split, I implemented the "Binning" approximation. I
                    divide the chosen axis into <strong>16 uniform
                        buckets</strong> (as shown in the course slides). I
                    iterate through all primitives, calculate which bucket their
                    centroid falls into, and update the bounding box and count
                    for each bucket.
                </li>
                <li>
                    <strong>Cost Evaluation:</strong>
                    I evaluate the SAH cost for the 15 possible split planes
                    between these buckets. The cost function is:
                    \[ C = C_{trav} + \frac{S_A}{S_{total}} N_A C_{isect} +
                    \frac{S_B}{S_{total}} N_B C_{isect} \]
                    where \(S\) represents surface area and \(N\) represents the
                    primitive count. I use efficient forward and backward scans
                    (prefix/suffix sums) to compute the surface areas and counts
                    for the left and right partitions in linear time.
                </li>
                <li>
                    <strong>Partitioning & Recursion:</strong>
                    After finding the split index with the minimum cost, I check
                    if splitting is actually beneficial compared to creating a
                    leaf. If it is, I use <code>std::partition</code> with a
                    lambda function to reorder the primitives in-place: those
                    falling into buckets left of the split point move to the
                    front, and the rest move to the back. Finally, I recursively
                    call <code>construct_bvh</code> on the left and right
                    segments.
                </li>
            </ol>

            <h3>Results: Rendering Large Models</h3>
            <p>
                With BVH acceleration, I can now render complex geometries with
                tens of thousands of triangles, which would have been impossible
                (or infinitely slow) with the naive implementation. Below are
                images of complex <code>.dae</code> files rendered with normal
                shading.
            </p>

            <div
                style="display: flex; flex-direction: column; align-items: center;">
                <table
                    style="width: 100%; text-align: center; border-collapse: collapse;">
                    <tr>
                        <td style="text-align: center;">
                            <img src="assets/part2/maxplanck.png"
                                width="400px" />
                            <figcaption>maxplanck.dae (Complex
                                Mesh)</figcaption>
                        </td>
                        <td style="text-align: center;">
                            <img src="assets/part2/CBlucy.png" width="400px" />
                            <figcaption>CBlucy.dae (High Poly
                                Count)</figcaption>
                        </td>
                    </tr>
                </table>
            </div>

            <h3>Performance Analysis</h3>
            <p>
                I compared the rendering performance of my BVH-accelerated path
                tracer against the naive implementation on scenes with varying
                geometric complexity. The tests were run on a single thread on
                Macbook Pro with an M2 chip.
            </p>

            <div align="center">
                <table border="1"
                    style="width: 80%; border-collapse: collapse;">
                    <tr>
                        <th style="padding: 10px;">Scene</th>
                        <th style="padding: 10px;">Triangle Count</th>
                        <th style="padding: 10px;">Render Time (No BVH)</th>
                        <th style="padding: 10px;">Render Time (With BVH)</th>
                    </tr>
                    <tr>
                        <td style="padding: 8px;">cow.dae</td>
                        <td style="padding: 8px;">5,856</td>
                        <td style="padding: 8px;">32.48 s </td> <td
                            style="padding: 8px;">0.0831 s </td> </tr>
                    <tr>
                        <td style="padding: 8px;">beetle.dae</td>
                        <td style="padding: 8px;">7,512</td>
                        <td style="padding: 8px;">38.96 s</td> <td
                            style="padding: 8px;">0.0728 s </td> </tr>
                    <tr>
                        <td style="padding: 8px;">CBdragon.dae</td>
                        <td style="padding: 8px;">105120</td>
                        <td style="padding: 8px;"> > 1 hour</td>
                        <td style="padding: 8px;">0.0899 s </td> </tr>
                </table>
            </div>

            <p>
                <strong>Analysis:</strong>
                The results demonstrate a drastic improvement in rendering
                speed. Without BVH, the intersection complexity is \(O(N)\) per
                ray, meaning the render time scales linearly with the number of
                triangles. For models like the Cow or Beetle, this is noticeably
                slow. With BVH, the complexity drops to \(O(\log N)\) as rays
                only traverse relevant nodes of the tree. This logarithmic
                scaling is evident in the data: even for the Dragon model with
                over 100,000 triangles, the rendering time remains sub-second,
                whereas the naive approach would take hours. The SAH heuristic
                further optimizes this by minimizing the probability of
                unnecessary intersections, producing a high-quality tree
                structure.
                <br>
                On the other hand, I found that in BVH, the render time does not
                increase significantly with the number of triangles, showing
                that <strong>the bottle neck has shifted from CPU computing
                    power to memory bandwidth and cache efficiency</strong>,
                which is a direction that can be further optimized in future
                work.
            </p>
            <h2>Part 3: Direct Illumination</h2>

            <h3>1. Walkthrough of Direct Lighting Implementations</h3>
            <p>In this part, I implemented two distinct algorithms for
                estimating direct illumination: <strong>Uniform Hemisphere
                    Sampling</strong> and <strong>Importance Light
                    Sampling</strong>.</p>

            <h4>Uniform Hemisphere Sampling</h4>
            <p>In the <code>estimate_direct_lighting_hemisphere</code> function,
                we estimate the radiance at an intersection point by uniformly
                sampling directions over the hemisphere:</p>
            <ul>
                <li><strong>Sampling Process:</strong> I used the
                    <code>hemisphereSampler</code> to generate a random
                    direction in object space and transformed it to world space
                    using the <code>o2w</code> matrix constructed from the
                    surface normal.</li>
                <li><strong>Ray Casting:</strong> A new ray is cast from the hit
                    point in the sampled direction. To prevent
                    self-intersection, the ray's <code>min_t</code> is set to
                    <code>EPS_D</code>.</li>
                <li><strong>Radiance Accumulation:</strong> If the ray hits a
                    light source (verified by <code>get_emission()</code>), its
                    contribution is added. This depends on the light's radiance,
                    the surface's BSDF evaluation, and the cosine of the angle
                    between the sample direction and the normal.</li>
                <li><strong>Monte Carlo Integration:</strong> The final result
                    is the average of all samples, scaled by the inverse of the
                    PDF for uniform hemisphere sampling, which is \(2\pi\).</li>
            </ul>

            <h4>Importance Light Sampling</h4>
            <p>In the <code>estimate_direct_lighting_importance</code> function,
                we sample directions directly toward the light sources, which
                significantly reduces variance and noise:</p>
            <ul>
                <li><strong>Iterating Lights:</strong> The function iterates
                    through every light in <code>scene->lights</code>. For delta
                    lights (like point lights), only one sample is taken; for
                    area lights, <code>ns_area_light</code> samples are
                    taken.</li>
                <li><strong>Visibility Testing:</strong> For each sampled
                    direction, a shadow ray is cast. The <code>max_t</code> of
                    this ray is set to <code>distToLight - EPS_D</code> to
                    ensure the ray does not erroneously intersect with the light
                    source itself.</li>
                <li><strong>Occlusion Check:</strong> I used
                    <code>bvh->has_intersection(shadow_ray)</code> for an
                    efficient visibility check. If the path is unoccluded, the
                    contribution is added based on the light's PDF, the BSDF,
                    and the cosine term.</li>
            </ul>

            <h3>2. Sampling Results Comparison</h3>
            <p>Below is a comparison of the Cornell Box rendered with both
                methods using the same sampling parameters:</p>
            <div align="middle">
                <table style="width:100%">
                    <tr align="center">
                        <td>
                            <img src="assets/part3/bunny_H_64_32.png"
                                align="middle" width="400px" />
                            <figcaption>Uniform Hemisphere Sampling</figcaption>
                        </td>
                        <td>
                            <img src="assets/part3/bunny_64_32.png"
                                align="middle" width="400px" />
                            <figcaption>Importance Light Sampling</figcaption>
                        </td>
                    </tr>
                </table>
            </div>

            <h3>3. Noise Levels in Soft Shadows (Light Sampling)</h3>
            <p>To demonstrate the effectiveness of importance sampling in
                rendering soft shadows, I rendered the <code>CBbunny.dae</code>
                scene with 1 sample per pixel (<code>-s 1</code>) while varying
                the number of light rays (<code>-l</code>):</p>
            <div align="middle">
                <table style="width:100%">
                    <tr align="center">
                        <td>
                            <img src="assets/part3/bunny_1_1.png" align="middle"
                                width="400px" />
                            <figcaption>1 Light Ray (-l 1)</figcaption>
                        </td>
                        <td>
                            <img src="assets/part3/bunny_1_4.png" align="middle"
                                width="400px" />
                            <figcaption>4 Light Rays (-l 4)</figcaption>
                        </td>
                    </tr>
                    <tr align="center">
                        <td>
                            <img src="assets/part3/bunny_1_16.png" align="middle"
                                width="400px" />
                            <figcaption>16 Light Rays (-l 16)</figcaption>
                        </td>
                        <td>
                            <img src="assets/part3/bunny_1_64.png" align="middle"
                                width="400px" />
                            <figcaption>64 Light Rays (-l 64)</figcaption>
                        </td>
                    </tr>
                </table>
            </div>
            <p><strong>Analysis:</strong> As the number of light rays increases
                from 1 to 64, the noise (graininess) in the soft shadow regions
                is drastically reduced. At \(l=1\), the shadows appear as a
                scattered collection of dots. By \(l=64\), the gradient of the
                soft shadow becomes very smooth as the Monte Carlo estimator
                gains more samples. Because we are sampling the lights directly,
                we achieve relatively stable results even with a low number of
                samples per pixel.</p>

            <h3>4. Comparison of Hemisphere and Importance Sampling</h3>
            <p>The comparison clearly demonstrates the superior efficiency of
                importance light sampling over uniform hemisphere sampling.
                Uniform hemisphere sampling is "blind" to light locations,
                casting many rays in directions that contribute zero radiance,
                leading to significant noise—especially in scenes with small
                light sources. Importance light sampling ensures that every
                shadow ray is directed toward a known light source, maximizing
                sample utility. This not only produces smoother soft shadows but
                also means that importance sampling requires far fewer samples
                to achieve the same visual quality as hemisphere sampling. In
                practice, importance sampling is the preferred method for direct
                lighting in path tracing due to its rapid convergence and
                reliability.</p>

            <h2>Part 4: Global Illumination</h2>
            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit...</p>

            <h2>Part 5: Adaptive Sampling</h2>
            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit...</p>

            <h2>(Optional) Part 6: Extra Credit Opportunities</h2>
            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit...</p>

        </div>
    </body>
</html>